{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/W4Rr5V6iwt5KHBt/Esef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataemperor/Shrimple/blob/chanmini/ShrimpDemandPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b5mqvIUbSUFj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# File paths for uploaded .txt files in Colab\n",
        "file_paths = {\n",
        "    \"Q1\": \"/content/Q1.txt\",\n",
        "    \"Q2\": \"/content/Q2.txt\",\n",
        "    \"Q3\": \"/content/Q3.txt\",\n",
        "      \"Q4\": \"/content/Q4.txt\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load a text file\n",
        "def load_txt(file_path, quarter):\n",
        "    df = pd.read_csv(file_path, sep=\"\\t\", engine=\"python\")  # Tab-separated data\n",
        "    df = df.melt(var_name=\"year\", value_name=\"demand\")  # Convert wide format to long format\n",
        "    df[\"year\"] = df[\"year\"].str.extract('(\\d{4})').astype(int)  # Extract the first year\n",
        "    df[\"quarter\"] = quarter  # Add quarter column\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "5TYH17y9SZao"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and merge all data\n",
        "dataframes = [load_txt(file_paths[q], i) for i, q in enumerate([\"Q1\", \"Q2\", \"Q3\", \"Q4\"], start=1)]\n",
        "data = pd.concat(dataframes, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "_ZODSWUwScMw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert to datetime format\n",
        "data[\"date\"] = pd.to_datetime(data[\"year\"].astype(str) + \"Q\" + data[\"quarter\"].astype(str))\n",
        "data = data.sort_values(by=\"date\").reset_index(drop=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ySagC7VSf8E",
        "outputId": "fc659168-a18b-421b-ea3a-6226a43f44c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3e1f80ddb26c>:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data[\"date\"] = pd.to_datetime(data[\"year\"].astype(str) + \"Q\" + data[\"quarter\"].astype(str))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Handle missing values\n",
        "data[\"demand\"] = pd.to_numeric(data[\"demand\"], errors='coerce')  # Convert demand to numeric\n",
        "data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xzarrDOSjlE",
        "outputId": "02aa8aa5-641e-4761-bf2f-2ecda374d02e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1e6139106f11>:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Feature Engineering\n",
        "data[\"lag_1\"] = data[\"demand\"].shift(1)  # Previous quarter demand\n",
        "data[\"lag_2\"] = data[\"demand\"].shift(2)  # Two quarters ago\n",
        "data[\"rolling_mean\"] = data[\"demand\"].rolling(window=4).mean()  # Average over 4 quarters\n"
      ],
      "metadata": {
        "id": "HQh0o01hSnJ3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Drop missing values (after shift operations)\n",
        "data.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "XpwuT7kvSn7o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Assuming 'lag_1', 'lag_2', and 'rolling_mean' are your features\n",
        "X = data[['lag_1', 'lag_2', 'rolling_mean']]\n",
        "y = data['demand']\n",
        "\n",
        "# Apply scaling to features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the scaled data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "id": "4VIUIjtPSqRj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a larger hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 500],  # Number of trees\n",
        "    'max_depth': [10, 20, 30, 50, None],    # Depth of the tree\n",
        "    'min_samples_split': [2, 5, 10, 20],    # Minimum samples required to split a node\n",
        "    'min_samples_leaf': [1, 2, 4, 10],      # Minimum samples required at leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider for split\n",
        "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestRegressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display best parameters\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# Use the best model to train\n",
        "model = grid_search.best_estimator_\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJsGoi9fSuof",
        "outputId": "da37d285-c1d1-4e16-8767-b5ca4f9f7aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1920 candidates, totalling 9600 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Initialize TimeSeriesSplit for cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Train the model using cross-validation\n",
        "scores = []\n",
        "for train_index, test_index in tscv.split(X_train):\n",
        "    X_train_cv, X_test_cv = X_train[train_index], X_train[test_index]\n",
        "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
        "\n",
        "    model.fit(X_train_cv, y_train_cv)\n",
        "    y_pred_cv = model.predict(X_test_cv)\n",
        "    mse_cv = mean_squared_error(y_test_cv, y_pred_cv)\n",
        "    scores.append(mse_cv)\n",
        "\n",
        "# Print average cross-validation score\n",
        "print(f\"Average MSE from Cross-Validation: {np.mean(scores)}\")\n"
      ],
      "metadata": {
        "id": "S2cC4r9pThwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Select important features based on feature importance\n",
        "sfm = SelectFromModel(model, threshold=0.05)  # 0.05 means keep features with importance greater than 5%\n",
        "sfm.fit(X_train, y_train)\n",
        "\n",
        "# Transform the data to select important features\n",
        "X_train_selected = sfm.transform(X_train)\n",
        "X_test_selected = sfm.transform(X_test)\n"
      ],
      "metadata": {
        "id": "JFFuLxZ9TnAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "iSlZboH1Tnpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate Model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "ax7t4O8ITq2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Future Predictions (Next 4 Quarters)\n",
        "future_year = data[\"year\"].max() + 1\n",
        "future_quarters = [1, 2, 3, 4]\n",
        "future_data = pd.DataFrame({\"year\": [future_year] * 4, \"quarter\": future_quarters})\n"
      ],
      "metadata": {
        "id": "DZKEFPQiTtxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate lag values from the last known data point\n",
        "latest_values = data.iloc[-1]\n",
        "future_data[\"lag_1\"] = [latest_values[\"demand\"]] + [np.nan] * 3\n",
        "future_data[\"lag_2\"] = [latest_values[\"lag_1\"]] + [latest_values[\"demand\"]] + [np.nan] * 2\n",
        "future_data[\"rolling_mean\"] = latest_values[\"rolling_mean\"]\n"
      ],
      "metadata": {
        "id": "W_3ypW_2Tv3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict Future Demand\n",
        "future_data.fillna(method=\"ffill\", inplace=True)  # Fill missing lag values\n",
        "features = ['lag_1', 'lag_2', 'rolling_mean']  # Define the features used in the model\n",
        "future_data[\"forecast\"] = model.predict(future_data[features])"
      ],
      "metadata": {
        "id": "a0H4mtQZTzhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display Forecast\n",
        "print(\"\\nFuture Demand Forecast:\")\n",
        "print(future_data[[\"year\", \"quarter\", \"forecast\"]])\n"
      ],
      "metadata": {
        "id": "plcoriBDT2G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data[\"date\"], data[\"demand\"], label=\"Actual Demand\")\n",
        "plt.plot(pd.date_range(start=data[\"date\"].max(), periods=5, freq='Q')[1:], future_data[\"forecast\"], label=\"Forecast\", linestyle=\"dashed\", color=\"red\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Shrimp Demand\")\n",
        "plt.title(\"Shrimp Demand Forecasting\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4sfs1YmmT4wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Calculate Evaluation Metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print Accuracy Results\n",
        "print(f\"ðŸ“Š Model Performance Metrics:\")\n",
        "print(f\"âœ… Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"âœ… Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"âœ… Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"âœ… RÂ² Score: {r2:.2f} (Closer to 1 means better model fit)\")\n"
      ],
      "metadata": {
        "id": "BRu3ebBYT639"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define a threshold to classify demand as \"High\" or \"Low\"\n",
        "threshold = np.median(y_test)  # You can adjust this based on business logic\n",
        "\n",
        "# Convert actual and predicted values to binary classes\n",
        "y_test_class = (y_test >= threshold).astype(int)\n",
        "y_pred_class = (y_pred >= threshold).astype(int)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test_class, y_pred_class)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "E5uORuGiT-de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, \"model.pkl\")\n",
        "\n",
        "# Save the scaler used for feature transformation\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "print(\"âœ… Model and Scaler saved successfully!\")\n"
      ],
      "metadata": {
        "id": "jh01RwvQUAi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}